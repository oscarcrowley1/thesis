{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import process_time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from prettytable import PrettyTable\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "from stgcn import STGCN, negLogLik_loss\n",
    "from utils import generate_dataset, load_scats_data, get_normalized_adj, print_save, generate_feature_vects, generate_weekday_feature_vects, generate_flow_only_feature_vects, generate_density_only_feature_vects\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "use_gpu = True #CHANGE FOR MY COMPUTER???\n",
    "num_timesteps_input = 26\n",
    "num_output = 1\n",
    "\n",
    "plot_rate = 20\n",
    "save_rate = 50\n",
    "\n",
    "# num_timesteps_input = 15\n",
    "# num_timesteps_output = 15\n",
    "\n",
    "epochs = 4000\n",
    "batch_size = 32\n",
    "dist_bool = False\n",
    "data_zip = \"bravoplus\" # alpha bravo bravoplus\n",
    "# all_days = True\n",
    "feature_vec_type = 0 # 0 all days. 1 weekdays only. 2 all days flow only. 3 all days density only\n",
    "load_model = False\n",
    "\n",
    "if not dist_bool:\n",
    "    num_output = 1\n",
    "else:\n",
    "    num_output = 2\n",
    "\n",
    "parser = argparse.ArgumentParser(description='STGCN')\n",
    "parser.add_argument('--enable-cuda', action='store_true',\n",
    "                    help='Enable CUDA')\n",
    "args = parser.parse_args()\n",
    "args.device = None\n",
    "#if args.enable_cuda and torch.cuda.is_available():\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    args.device = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Device: {args.device}\")\n",
    "\n",
    "\n",
    "def train_epoch(training_input, training_target, batch_size):\n",
    "    \"\"\"\n",
    "    Trains one epoch with the given data.\n",
    "    :param training_input: Training inputs of shape (num_samples, num_nodes,\n",
    "    num_timesteps_train, num_features).\n",
    "    :param training_target: Training targets of shape (num_samples, num_nodes,\n",
    "    num_timesteps_predict).\n",
    "    :param batch_size: Batch size to use during training.\n",
    "    :return: Average loss for this epoch.\n",
    "    \"\"\"\n",
    "    permutation = torch.randperm(training_input.shape[0])\n",
    "\n",
    "    if dist_bool:\n",
    "        epoch_training_log_losses = []\n",
    "        \n",
    "    epoch_training_classic_losses = []\n",
    "\n",
    "    for i in range(0, training_input.shape[0], batch_size):\n",
    "        print(\"Batch: {}\".format(i), end='\\r')\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        X_batch, y_batch = training_input[indices], training_target[indices]\n",
    "        X_batch = X_batch.to(device=args.device)\n",
    "        y_batch = y_batch.to(device=args.device)\n",
    "\n",
    "        out = net(A_wave, X_batch)\n",
    "        #print(f\"OUTTYPE:\\t{out.type}\")\n",
    "        #loss = loss_criterion(out, y_batch)\n",
    "        # print(out[..., 0].shape)\n",
    "        # print(y_batch.shape)\n",
    "        \n",
    "        \n",
    "        if dist_bool:\n",
    "            log_loss = negLogLik_loss(out, y_batch)\n",
    "            classic_loss = loss_criterion(out[:, :, 0][..., None], y_batch)\n",
    "            epoch_training_log_losses.append(log_loss.detach().cpu().numpy())\n",
    "            log_loss.backward()\n",
    "        else:\n",
    "            classic_loss = loss_criterion(out, y_batch)\n",
    "            classic_loss.backward()\n",
    "            \n",
    "        epoch_training_classic_losses.append(classic_loss.detach().cpu().numpy())\n",
    "            \n",
    "        optimizer.step()\n",
    "    print(\"Batch: {}\".format(i))\n",
    "    print(\"Finished Loops\")\n",
    "    if dist_bool:\n",
    "        average_loss_s = sum(epoch_training_log_losses)/len(epoch_training_log_losses), sum(epoch_training_classic_losses)/len(epoch_training_classic_losses)\n",
    "    else:\n",
    "        average_loss_s = sum(epoch_training_classic_losses)/len(epoch_training_classic_losses)\n",
    "\n",
    "    return average_loss_s\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    print_save(f, \"\\nPARAMETERS:\")\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        #print_save(f, f\"{name}:\\t{parameter}\")\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print_save(f, f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    f = open(\"runs/run_info.txt\", \"w\")\n",
    "# info_string = \"Epsilon:\\t\" + str(epsilon) + \"\\nDelta Squared:\\t\" + str(delta_squared) + \"\\nUses these distances\\n\" + str(coord_array)\n",
    "\n",
    "# f.close()\n",
    "    print_save(f, \"Begin Setup\")\n",
    "    rand_seed = 8 # was 7\n",
    "    print_save(f, f\"Random Seed:\\t{rand_seed}\")\n",
    "    torch.manual_seed(rand_seed)\n",
    "\n",
    "    print_save(f, f\"Input Timesteps:\\t{num_timesteps_input}\")\n",
    "    print_save(f, f\"Output Timesteps:\\t{num_output}\")\n",
    "    print_save(f, f\"Use GPU:\\t{use_gpu}\")\n",
    "    print_save(f, f\"Plot Rate:\\t{plot_rate}\")\n",
    "    print_save(f, f\"Epochs:\\t{epochs}\")\n",
    "    print_save(f, f\"Batch Size:\\t{batch_size}\")\n",
    "    print_save(f, f\"Distribution:\\t{dist_bool}\")\n",
    "    print_save(f, f\"Load Model:\\t{load_model}\")\n",
    "    print_save(f, f\"Feature Vector Type:\\t{feature_vec_type}\")\n",
    "\n",
    "    A, X, means, stds, info_string = load_scats_data(data_zip)\n",
    "\n",
    "    print_save(f, info_string)\n",
    "\n",
    "    #print_save(f, A)\n",
    "\n",
    "    # split_line1 = int(X.shape[2] * 0.1)#0.6\n",
    "    # split_line2 = int(X.shape[2] * 0.15)#0.8\n",
    "    # split_line3 = int(X.shape[2] * 0.2)\n",
    "\n",
    "    print_save(f, \"Split Data\")\n",
    "    \n",
    "    # total_input, total_target, num_timesteps_input = generate_feature_vects(X)\n",
    "\n",
    "    if feature_vec_type == 0:\n",
    "        training_input, training_target, val_input, val_target, test_input, test_target, num_timesteps_input = generate_feature_vects(X)\n",
    "    elif feature_vec_type == 1:\n",
    "        training_input, training_target, val_input, val_target, test_input, test_target, num_timesteps_input = generate_weekday_feature_vects(X)\n",
    "    elif feature_vec_type == 2:\n",
    "        training_input, training_target, val_input, val_target, test_input, test_target, num_timesteps_input = generate_flow_only_feature_vects(X)\n",
    "    elif feature_vec_type == 3:\n",
    "        training_input, training_target, val_input, val_target, test_input, test_target, num_timesteps_input = generate_density_only_feature_vects(X)\n",
    "\n",
    "    print_save(f, \"Shuffle Data\")\n",
    "\n",
    "    # test_indx = np.arange((480*36), (480*45)) # day 36 mon to 44 tues inclusive\n",
    "\n",
    "    # before_indx = np.arange((480*36))\n",
    "    # after_indx = np.arange((480*45), total_input.shape[0])\n",
    "\n",
    "    # print(before_indx)\n",
    "    # print(after_indx)\n",
    "\n",
    "    # other_indx = np.concatenate((before_indx, after_indx))\n",
    "    # print(other_indx)\n",
    "    # np.random.shuffle(other_indx)\n",
    "    # split_line = int(other_indx.shape[0] * (5/9))\n",
    "    # training_indx = other_indx[:split_line]\n",
    "    # val_indx = other_indx[split_line:]\n",
    "\n",
    "    # test_indx = np.arange((480*36), (480*45)) # day 36 mon to 44 tues inclusive\n",
    "\n",
    "    # other_indx = np.concatenate(np.arange((480*36)), np.arange((480*45), total_input.shape[0]))\n",
    "    # np.random.shuffle(other_indx)\n",
    "    # split_line = int(other_indx.shape * (5/9))\n",
    "    # training_indx = other_indx[:split_line]\n",
    "    # val_indx = other_indx[split_line:]\n",
    "\n",
    "    # rand_indx = torch.randperm(total_input.shape[0])\n",
    "\n",
    "    # split_line1 = int(total_input.shape[0] * 0.6)#0.6\n",
    "    # split_line2 = int(total_input.shape[0] * 0.9)#0.8\n",
    "\n",
    "    # split_line1 = int(total_input.shape[0] * 0.5)#0.6\n",
    "    # split_line2 = int(total_input.shape[0] * 0.9)#0.8\n",
    "\n",
    "    # training_indx = rand_indx[:split_line1]\n",
    "    # val_indx = rand_indx[split_line1:split_line2]\n",
    "    # test_indx = rand_indx[split_line2:]\n",
    "\n",
    "    # training_input = total_input[:split_line1, :, :, :]\n",
    "    # training_target = total_target[:split_line1, :, :]\n",
    "\n",
    "    # val_input = total_input[split_line1:split_line2, :, :, :]\n",
    "    # val_target = total_target[split_line1:split_line2, :, :]\n",
    "\n",
    "    # test_input = total_input[split_line2:, :, :, :]\n",
    "    # test_target = total_target[split_line2:, :, :]\n",
    "\n",
    "    # training_input = total_input[training_indx, :, :, :]\n",
    "    # training_target = total_target[training_indx, :, :]\n",
    "\n",
    "    # val_input = total_input[val_indx, :, :, :]\n",
    "    # val_target = total_target[val_indx, :, :]\n",
    "\n",
    "    # test_input = total_input[test_indx, :, :, :]\n",
    "    # test_target = total_target[test_indx, :, :]\n",
    "\n",
    "    # split_line1 = int(X.shape[2] * 0.6)#0.6\n",
    "    # split_line2 = int(X.shape[2] * 0.9)#0.8\n",
    "\n",
    "    # train_original_data = X[:, :, :split_line1]\n",
    "    # val_original_data = X[:, :, split_line1:split_line2]\n",
    "    # # test_original_data = X[:, :, split_line2:split_line3]\n",
    "    # test_original_data = X[:, :, split_line2:]\n",
    "\n",
    "    # training_input, training_target = generate_dataset(train_original_data,\n",
    "    #                                                    num_timesteps_input=num_timesteps_input,\n",
    "    #                                                    num_timesteps_output=num_timesteps_output)\n",
    "    # val_input, val_target = generate_dataset(val_original_data,\n",
    "    #                                          num_timesteps_input=num_timesteps_input,\n",
    "    #                                          num_timesteps_output=num_timesteps_output)\n",
    "    # test_input, test_target = generate_dataset(test_original_data,\n",
    "    #                                            num_timesteps_input=num_timesteps_input,\n",
    "    #                                            num_timesteps_output=num_timesteps_output)\n",
    "\n",
    "    # print(f\"SHapes: {training_input.shape}, {training_input_test.shape}\")\n",
    "\n",
    "    print_save(f, \"Normalise Adjacency Matrix\")\n",
    "\n",
    "    A_wave = get_normalized_adj(A)\n",
    "\n",
    "    #print_save(f, A_wave)\n",
    "    A_wave = torch.from_numpy(A_wave)\n",
    "\n",
    "    A_wave = A_wave.to(device=args.device)\n",
    "\n",
    "    print_save(f, \"Initialise STGCN\")\n",
    "\n",
    "    net = STGCN(A_wave.shape[0],\n",
    "                training_input.shape[3],\n",
    "                num_timesteps_input,\n",
    "                num_output).to(device=args.device)\n",
    "\n",
    "\n",
    "    if load_model == True:\n",
    "        model_string = \"final_models/bravo_d03_nodist_2503_e15d10_PRUNING/model_0330_1703_e999_out1\"\n",
    "        net.load_state_dict(torch.load(model_string))\n",
    "\n",
    "    # writer.add_graph(net, (A_wave, val_input))\n",
    "    \n",
    "    # writer.flush()\n",
    "    # writer.close()\n",
    "    # sys.exit()\n",
    "    \n",
    "    print_save(f, str(count_parameters(net)))\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    loss_criterion = nn.MSELoss()\n",
    "    #loss_criterion = negLogLik_loss()\n",
    "\n",
    "    if dist_bool:\n",
    "        training_log_losses = []\n",
    "        validation_log_losses = []\n",
    "\n",
    "    training_classic_losses = []\n",
    "    validation_classic_losses = []\n",
    "\n",
    "    validation_maes = []\n",
    "    validation_mses = []\n",
    "\n",
    "    #output_array = []\n",
    "\n",
    "    print_save(f, \"Begin Training\")\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    training_start = process_time()\n",
    "    x_epoch_start = process_time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = process_time()\n",
    "        print(\"Epoch Number: {}\".format(epoch))\n",
    "        print(\"Epoch Number: {}\".format(epoch))\n",
    "        if dist_bool:\n",
    "            log_loss, classic_loss = train_epoch(training_input, training_target,\n",
    "                            batch_size=batch_size)\n",
    "        else:\n",
    "            classic_loss = train_epoch(training_input, training_target,\n",
    "                            batch_size=batch_size)\n",
    "        print(\"Returned Losses\")\n",
    "        if dist_bool:\n",
    "            training_log_losses.append(log_loss)\n",
    "            writer.add_scalar(\"Training Log Loss\", log_loss, epoch)\n",
    "        \n",
    "        training_classic_losses.append(classic_loss)\n",
    "        writer.add_scalar(\"Training Classic Loss\", classic_loss, epoch)\n",
    "        \n",
    "        # Run validation\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            val_input = val_input.to(device=args.device)\n",
    "            val_target = val_target.to(device=args.device)\n",
    "\n",
    "            out = net(A_wave, val_input)\n",
    "            #val_loss = loss_criterion(out, val_target).to(device=\"cpu\")\n",
    "            if dist_bool:\n",
    "                val_log_loss = negLogLik_loss(out, val_target).to(device=\"cpu\")\n",
    "                validation_log_losses.append((val_log_loss.detach().numpy()).item())\n",
    "                val_classic_loss = loss_criterion(out[:, :, 0][..., None], val_target).to(device=\"cpu\")\n",
    "            else:\n",
    "                val_classic_loss = loss_criterion(out, val_target).to(device=\"cpu\")\n",
    "            #validation_losses.append(np.asscalar(val_loss.detach().numpy()))\n",
    "            validation_classic_losses.append((val_classic_loss.detach().numpy()).item())\n",
    "\n",
    "            # out_unnormalized = out.detach().cpu().numpy()*stds[0]+means[0]\n",
    "            # target_unnormalized = val_target.detach().cpu().numpy()*stds[0]+means[0]\n",
    "\n",
    "\n",
    "            # # if (epoch+1)%plot_rate==0:\n",
    "            # #     plt.plot(out_unnormalized[:, 0, 2], label=\"Out\")\n",
    "            # #     plt.plot(target_unnormalized[:, 0, 2], label=\"Target\")\n",
    "            # #     plt.legend()\n",
    "            # #     plt.show()\n",
    "\n",
    "            # mae = np.mean(np.absolute(out_unnormalized - target_unnormalized)) #why would mae be calculated after normalisation\n",
    "            # rmse = np.sqrt(np.mean((out_unnormalized - target_unnormalized)**2))\n",
    "\n",
    "            #mae_15min = np.mean(np.absolute(out_unnormalized[:,:,4] - target_unnormalized[:,:,4]))\n",
    "\n",
    "            # validation_maes.append(mae) 3854, 281373\n",
    "\n",
    "            out = None\n",
    "            val_input = val_input.to(device=\"cpu\")\n",
    "            val_target = val_target.to(device=\"cpu\")\n",
    "        \n",
    "        if dist_bool:\n",
    "            writer.add_scalar(\"Validation Log Loss\", val_log_loss, epoch)\n",
    "        \n",
    "        writer.add_scalar(\"Validation Classic Loss\", val_classic_loss, epoch)\n",
    "        # writer.add_scalar(\"Validation MAE\", mae, epoch)\n",
    "        # #writer.add_scalar(\"Validation MAE 15th min\", rmse, epoch)\n",
    "        # writer.add_scalar(\"Validation MSE Unnormalised\", rmse, epoch)\n",
    "        if dist_bool:\n",
    "            print(\"Training loss: {}\".format(training_log_losses[-1]))\n",
    "            print(\"Validation loss: {}\".format(validation_log_losses[-1]))\n",
    "        else:\n",
    "            print(\"Training loss: {}\".format(training_classic_losses[-1]))\n",
    "            print(\"Validation loss: {}\".format(validation_classic_losses[-1]))\n",
    "        #print(\"Validation MAE: {}\".format(validation_maes[-1]))\n",
    "        #print(f\"THE LENGTHS: {training_losses}\\t{validation_losses}\\t{validation_maes}\")\n",
    "        # if (epoch+1)%plot_rate==0:\n",
    "        #     x_epoch_end = process_time()\n",
    "        #     print(f\"Time for {plot_rate} epochs:\\t{x_epoch_end-x_epoch_start}\")\n",
    "        #     x_epoch_start = x_epoch_end\n",
    "        #     plt.plot(training_losses, label=\"training loss\")\n",
    "        #     plt.plot(validation_losses, label=\"validation loss\")\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "        #     print(\"PLOTTED\")\n",
    "        if (epoch+1) % save_rate == 0:\n",
    "            now = datetime.now()\n",
    "            time_string = now.strftime(\"%m%d_%H%M\") + \"_e\" + str(epoch) + \"_out\" + str(num_output)\n",
    "\n",
    "            torch.save(net.state_dict(), (\"saved_models/model_\" + time_string))\n",
    "            shutil.copy(\"runs/run_info.txt\", \"saved_models/run_info_\" + time_string)\n",
    "\n",
    "        checkpoint_path = \"checkpoints/\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            os.makedirs(checkpoint_path)\n",
    "        with open(\"checkpoints/losses.pk\", \"wb\") as fd:\n",
    "            if dist_bool:\n",
    "                pk.dump((training_log_losses, validation_log_losses, validation_maes), fd)\n",
    "            else:\n",
    "                pk.dump((training_classic_losses, validation_classic_losses, validation_maes), fd)\n",
    "\n",
    "        epoch_stop = process_time()\n",
    "        epoch_length = epoch_stop-epoch_start\n",
    "        print(f\"Epoch Time:\\t{epoch_length}\")\n",
    "        writer.add_scalar(\"Epoch Length\", epoch_length, epoch)\n",
    "            \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    now = datetime.now()\n",
    "    time_string = now.strftime(\"%m%d_%H%M\") + \"_e\" + str(epoch) + \"_out\" + str(num_output)\n",
    "\n",
    "    torch.save(net.state_dict(), (\"saved_models/model_\" + time_string))\n",
    "\n",
    "    training_stop = process_time()\n",
    "    print(f\"Training Time:\\t{training_stop-training_start}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
